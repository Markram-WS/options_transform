{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "76a170b6-8d57-4dcf-95d5-c4eb911052ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clickhouse-connect in /venv/lib/python3.10/site-packages (0.8.6)\n",
      "Requirement already satisfied: certifi in /venv/lib/python3.10/site-packages (from clickhouse-connect) (2024.8.30)\n",
      "Requirement already satisfied: urllib3>=1.26 in /venv/lib/python3.10/site-packages (from clickhouse-connect) (2.2.3)\n",
      "Requirement already satisfied: pytz in /venv/lib/python3.10/site-packages (from clickhouse-connect) (2024.2)\n",
      "Requirement already satisfied: zstandard in /venv/lib/python3.10/site-packages (from clickhouse-connect) (0.23.0)\n",
      "Requirement already satisfied: lz4 in /venv/lib/python3.10/site-packages (from clickhouse-connect) (4.3.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: clickhouse-driver in /venv/lib/python3.10/site-packages (0.2.9)\n",
      "Requirement already satisfied: pytz in /venv/lib/python3.10/site-packages (from clickhouse-driver) (2024.2)\n",
      "Requirement already satisfied: tzlocal in /venv/lib/python3.10/site-packages (from clickhouse-driver) (5.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install clickhouse-connect\n",
    "!pip install clickhouse-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "835084dc-a001-488b-a521-3b13470a6fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/workspace/options_transform\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c0804100-f012-468a-b46b-b7acf50a66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from IPython.display import clear_output\n",
    "import joblib\n",
    "import random\n",
    "import hashlib\n",
    "import uuid\n",
    "from tqdm.notebook import  tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e624f80-11ce-4bf3-a75e-9feee10980cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler:\n",
    "    def __init__(self, keys, create=False, path=\"./data/scaler/\"):\n",
    "        self.scaler = {}\n",
    "        # self._scale = scale\n",
    "        self._keys = keys\n",
    "        self._path = path\n",
    "        self._creatScaler(create)\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.scaler\n",
    "\n",
    "    @property\n",
    "    def keys(self):\n",
    "        return self._keys\n",
    "\n",
    "    def _creatScaler(self, create):\n",
    "        if not os.path.exists(self._path):\n",
    "            os.makedirs(self._path)\n",
    "\n",
    "        dir_list = os.listdir(self._path)\n",
    "\n",
    "        for k in self._keys:\n",
    "            self.scaler[k] = None\n",
    "            for dir in dir_list:\n",
    "                if k in dir:\n",
    "                    print(f\"[Load-{k}] : \", self._path + dir)\n",
    "                    self.scaler[k] = joblib.load(self._path + dir)\n",
    "\n",
    "            if k not in self.scaler.keys() or create:\n",
    "                print(\n",
    "                    f\"[Set] : [{k}] - {StandardScaler()} \",\n",
    "                )\n",
    "                self.scaler[k] = StandardScaler()\n",
    "\n",
    "    def save(self):\n",
    "        for k in self.scaler.keys():\n",
    "            joblib.dump(self.scaler[k], self._path + f\"scaler_{k}.gz\")\n",
    "\n",
    "    def groupTransform(self, c, data, **kwargs):\n",
    "        # ------------------ customize --------------------\n",
    "        if c in kwargs[\"QUOTE_COL\"]:\n",
    "            return self.scaler[\"QUOTE\"].transform(data)\n",
    "        elif c in kwargs[\"VEGA_COL\"]:\n",
    "            return self.scaler[\"VEGA\"].transform(data)\n",
    "        elif c in kwargs[\"VOLUME_COL\"]:\n",
    "            return self.scaler[\"VOLUME\"].transform(data)\n",
    "        else:\n",
    "            return self.scaler[c].transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "055229cf-0832-4304-bad0-fad857bf8984",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH     = '/app/raw/OptionsEOD.csv/'\n",
    "##create @ part 1\n",
    "##PARQUET_PATH = './data/OptionsEOD.parquet'\n",
    "##create @ part 2\n",
    "##PARQUET_STG_PATH = './data/OptionsEOD_STG.parquet'\n",
    "SCALER_COL  = [ 'UNDERLYING_LAST','STRIKE','STRIKE_DISTANCE','INTRINSIC_VALUE','DTE','C_VEGA','P_VEGA',\t'C_BID',\t'C_ASK', 'C_VOLUME',  'P_BID',\t'P_ASK', 'P_VOLUME' ]\n",
    "#create @ part 3\n",
    "OTHER_COL = ['SYMBOL','QUOTE_UNIXTIME', 'EXPIRE_UNIX']\n",
    "DATETIME_COLUMNS = ['EXPIRE_DATE', 'QUOTE_DATE', 'QUOTE_READTIME']\n",
    "export_dtype = {\n",
    "    'SYMBOL':str,\n",
    "    'QUOTE_UNIXTIME':int,\n",
    "    'EXPIRE_UNIX':int,\n",
    "    'UNDERLYING_LAST':float,\n",
    "    'STRIKE':float,\n",
    "    'STRIKE_DISTANCE':float,\n",
    "    'INTRINSIC_VALUE':float,\n",
    "    'DTE':float,\n",
    "    'C_VEGA':float,\n",
    "    'P_VEGA':float,\t\n",
    "    'C_BID':float,\t\n",
    "    'C_ASK':float, \n",
    "    'C_VOLUME':float,  \n",
    "    'P_BID':float,\t\n",
    "    'P_ASK':float, \n",
    "    'P_VOLUME':float\n",
    "}\n",
    "\n",
    "QUOTE_COL = ['C_BID',\t'C_ASK',  'P_BID',\t'P_ASK']\n",
    "VEGA_COL =  [\"C_VEGA\",\"P_VEGA\"] \n",
    "VOLUME_COL =  [\"TOTAL_VOLUME\",\"C_VOLUME\",\"P_VOLUME\"] \n",
    "\n",
    "SCALER_QUOTE_COL = [i for i,v in enumerate(SCALER_COL) if v in QUOTE_COL]\n",
    "SCALER_VEGA_COL = [i for i,v in enumerate(SCALER_COL) if v in VEGA_COL]\n",
    "SCALER_VOLUME_COL = [i for i,v in enumerate(SCALER_COL) if v in VOLUME_COL]\n",
    "SCALER_OTHER_COL = [i for i,v in enumerate(SCALER_COL) if v not in QUOTE_COL+VEGA_COL+VOLUME_COL ]\n",
    "\n",
    "\n",
    "UNIQUE_KEYS = ['SYMBOL','EXPIRE_DATE']\n",
    "H5_PATH = './data/OptTrainRenforcement/'\n",
    "START = True#True#'2010-09'#True\n",
    "BUCKET = 'tensorflow'\n",
    "\n",
    "MAX_OPTIONS_LEN = 16\n",
    "RM_STRIKE_ = 2\n",
    "#==== Create options_qoute\n",
    "options_qoute = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a086f1a-d4ac-4c40-aec4-28dcc78f68f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[QUOTE_UNIXTIME]</th>\n",
       "      <th>[QUOTE_READTIME]</th>\n",
       "      <th>[QUOTE_DATE]</th>\n",
       "      <th>[QUOTE_TIME_HOURS]</th>\n",
       "      <th>[UNDERLYING_LAST]</th>\n",
       "      <th>[EXPIRE_DATE]</th>\n",
       "      <th>[EXPIRE_UNIX]</th>\n",
       "      <th>[DTE]</th>\n",
       "      <th>[C_DELTA]</th>\n",
       "      <th>[C_GAMMA]</th>\n",
       "      <th>...</th>\n",
       "      <th>[P_LAST]</th>\n",
       "      <th>[P_DELTA]</th>\n",
       "      <th>[P_GAMMA]</th>\n",
       "      <th>[P_VEGA]</th>\n",
       "      <th>[P_THETA]</th>\n",
       "      <th>[P_RHO]</th>\n",
       "      <th>[P_IV]</th>\n",
       "      <th>[P_VOLUME]</th>\n",
       "      <th>[STRIKE_DISTANCE]</th>\n",
       "      <th>[STRIKE_DISTANCE_PCT]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1325624400</td>\n",
       "      <td>2012-01-03 16:00</td>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>16.0</td>\n",
       "      <td>56.9</td>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003130</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>-0.004080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.793590</td>\n",
       "      <td></td>\n",
       "      <td>10.9</td>\n",
       "      <td>0.192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1325624400</td>\n",
       "      <td>2012-01-03 16:00</td>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>16.0</td>\n",
       "      <td>56.9</td>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003580</td>\n",
       "      <td>0.002780</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>-0.004420</td>\n",
       "      <td>-0.000420</td>\n",
       "      <td>0.721270</td>\n",
       "      <td></td>\n",
       "      <td>9.9</td>\n",
       "      <td>0.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1325624400</td>\n",
       "      <td>2012-01-03 16:00</td>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>16.0</td>\n",
       "      <td>56.9</td>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.004040</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>-0.004350</td>\n",
       "      <td>-0.000430</td>\n",
       "      <td>0.648640</td>\n",
       "      <td></td>\n",
       "      <td>8.9</td>\n",
       "      <td>0.156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1325624400</td>\n",
       "      <td>2012-01-03 16:00</td>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>16.0</td>\n",
       "      <td>56.9</td>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>-0.004390</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>-0.004050</td>\n",
       "      <td>-0.000330</td>\n",
       "      <td>0.577770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1325624400</td>\n",
       "      <td>2012-01-03 16:00</td>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>16.0</td>\n",
       "      <td>56.9</td>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.971540</td>\n",
       "      <td>0.016360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>-0.004860</td>\n",
       "      <td>0.005270</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>-0.004140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.507560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   [QUOTE_UNIXTIME]   [QUOTE_READTIME]  [QUOTE_DATE]   [QUOTE_TIME_HOURS]  \\\n",
       "0        1325624400   2012-01-03 16:00    2012-01-03                 16.0   \n",
       "1        1325624400   2012-01-03 16:00    2012-01-03                 16.0   \n",
       "2        1325624400   2012-01-03 16:00    2012-01-03                 16.0   \n",
       "3        1325624400   2012-01-03 16:00    2012-01-03                 16.0   \n",
       "4        1325624400   2012-01-03 16:00    2012-01-03                 16.0   \n",
       "\n",
       "    [UNDERLYING_LAST]  [EXPIRE_DATE]   [EXPIRE_UNIX]   [DTE]  [C_DELTA]  \\\n",
       "0                56.9     2012-01-06      1325883600     3.0   1.000000   \n",
       "1                56.9     2012-01-06      1325883600     3.0   1.000000   \n",
       "2                56.9     2012-01-06      1325883600     3.0   1.000000   \n",
       "3                56.9     2012-01-06      1325883600     3.0   1.000000   \n",
       "4                56.9     2012-01-06      1325883600     3.0   0.971540   \n",
       "\n",
       "   [C_GAMMA]  ...   [P_LAST]   [P_DELTA]  [P_GAMMA]   [P_VEGA]   [P_THETA]  \\\n",
       "0   0.000000  ...   0.000000   -0.003130   0.002320   0.000350   -0.004080   \n",
       "1   0.000000  ...   0.000000   -0.003580   0.002780   0.000530   -0.004420   \n",
       "2   0.000000  ...   0.000000   -0.004040   0.003340   0.000950   -0.004350   \n",
       "3   0.000000  ...   0.010000   -0.004390   0.004150   0.001170   -0.004050   \n",
       "4   0.016360  ...   0.010000   -0.004860   0.005270   0.001370   -0.004140   \n",
       "\n",
       "      [P_RHO]     [P_IV]  [P_VOLUME]  [STRIKE_DISTANCE]  \\\n",
       "0    0.000000   0.793590                           10.9   \n",
       "1   -0.000420   0.721270                            9.9   \n",
       "2   -0.000430   0.648640                            8.9   \n",
       "3   -0.000330   0.577770    0.000000                7.9   \n",
       "4    0.000000   0.507560    0.000000                6.9   \n",
       "\n",
       "    [STRIKE_DISTANCE_PCT]  \n",
       "0                   0.192  \n",
       "1                   0.174  \n",
       "2                   0.156  \n",
       "3                   0.139  \n",
       "4                   0.121  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example data\n",
    "EOD_CSV = pd.read_csv(CSV_PATH+\"qqq/qqq_eod_201201.txt\", engine='pyarrow')\n",
    "EOD_CSV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ad64879-ee04-4880-8f8e-cc420c34afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create \n",
    "import clickhouse_connect  \n",
    "clickhouse_client = clickhouse_connect.get_client(host='host.containers.internal', port=8123, database='stagging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2b457d-1015-4165-abf7-685d20d29e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## STAGGING #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "065d4514-7b36-4686-87e6-b72444e37c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eedf6bee72846b5b1305355d591d52a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SYMBOL-qqq:   0%|          | 0/145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] : /app/raw/OptionsEOD.csv/qqq/qqq_eod_202312.txt        \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e775218968064169bc6acf3ac4c372ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SYMBOL-spy:   0%|          | 0/169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] : /app/raw/OptionsEOD.csv/spy/spy_eod_202309.txt        \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0bcce5e42dc446eb6ecda50a30f7ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SYMBOL-spx:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] : /app/raw/OptionsEOD.csv/spx/spx_eod_202312.txt        \r"
     ]
    }
   ],
   "source": [
    "def transform_I() \n",
    "    for d in os.listdir(CSV_PATH) :\n",
    "        for f in tqdm( os.listdir(CSV_PATH+f\"{d}/\" )[:] , desc=f\"SYMBOL-{d}\" ):\n",
    "            if f.endswith(\".txt\"):\n",
    "                ## load\n",
    "                print( f\"[LOAD] : {CSV_PATH}{d}/{f}        \",end='\\r')\n",
    "                \n",
    "                EOD_CSV = pd.read_csv(CSV_PATH+f\"{d}/\"+f, engine='pyarrow' )\n",
    "                    \n",
    "                ## rename col.\n",
    "                for c in EOD_CSV.columns:\n",
    "                    EOD_CSV = EOD_CSV.rename( columns={ c:c.strip().replace(']','').replace('[','') } )\n",
    "                \n",
    "                ## add symbol \n",
    "                EOD_CSV['SYMBOL'] = d.upper()\n",
    "                \n",
    "                ## add INTRINSIC_VALUE\n",
    "                EOD_CSV['INTRINSIC_VALUE'] = EOD_CSV['UNDERLYING_LAST'] - EOD_CSV['STRIKE']\n",
    "                \n",
    "                EOD_CSV.dropna(subset=SCALER_COL)\n",
    "    \n",
    "    \n",
    "                # date columns convert to datetime\n",
    "                for c in [\"QUOTE_READTIME\",\"QUOTE_DATE\",\"EXPIRE_DATE\"]:\n",
    "                    EOD_CSV[c] = pd.to_datetime(EOD_CSV[c])\n",
    "                # # Convert datetime columns to string using strftime\n",
    "                # EOD_CSV['EXPIRE_DATE'] = EOD_CSV['EXPIRE_DATE'].dt.strftime('%Y-%m-%d')\n",
    "                # EOD_CSV['QUOTE_DATE'] = EOD_CSV['QUOTE_DATE'].dt.strftime('%Y-%m-%d')\n",
    "                # EOD_CSV['QUOTE_READTIME'] = EOD_CSV['QUOTE_READTIME'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                \n",
    "    \n",
    "                \n",
    "                #clean float data\n",
    "                for c in ['INTRINSIC_VALUE','C_DELTA','C_GAMMA','C_VEGA','C_THETA','C_RHO','C_IV','C_VOLUME','C_LAST','C_BID','C_ASK','STRIKE','P_BID','P_ASK','P_LAST','P_DELTA','P_GAMMA','P_VEGA','P_THETA','P_RHO','P_IV','P_VOLUME','STRIKE_DISTANCE','STRIKE_DISTANCE_PCT']:\n",
    "                    if EOD_CSV[c].dtype not in ( 'float32','float64'):\n",
    "                        EOD_CSV[c] = EOD_CSV[c].apply(lambda x: x.strip())\n",
    "                        EOD_CSV[c] = EOD_CSV[c].replace('', np.nan).fillna(np.nan)\n",
    "                        EOD_CSV[c] = EOD_CSV[c].astype('float64')\n",
    "                    if EOD_CSV[c].dtype == 'float32':\n",
    "                        EOD_CSV[c] = EOD_CSV[c].astype('float64')\n",
    "    \n",
    "                # Convert any necessary columns to the appropriate types for ClickHouse\n",
    "                EOD_CSV['C_SIZE'] = EOD_CSV['C_SIZE'].astype(str)\n",
    "                EOD_CSV['P_SIZE'] = EOD_CSV['P_SIZE'].astype(str)\n",
    "                EOD_CSV['SYMBOL'] = EOD_CSV['SYMBOL'].astype(str)\n",
    "                \n",
    "                \n",
    "                # For columns that need UInt32 or UInt64, also handle NaN or invalid values\n",
    "                EOD_CSV['QUOTE_UNIXTIME'] = EOD_CSV['QUOTE_UNIXTIME'].fillna(0).astype('UInt32')  # Fill NaN with 0 for UInt32 columns\n",
    "                EOD_CSV['EXPIRE_UNIX'] = EOD_CSV['EXPIRE_UNIX'].fillna(0).astype('UInt32')\n",
    "    \n",
    "                EOD_CSV['C_VOLUME'] = EOD_CSV['C_VOLUME'].fillna(0).astype('UInt64')  # Convert to UInt64\n",
    "                EOD_CSV['P_VOLUME'] = EOD_CSV['P_VOLUME'].fillna(0).astype('UInt64')\n",
    "                \n",
    "                # For columns that should be of type float64 (for example, 'C_LAST', 'DTE'), you can use:\n",
    "                EOD_CSV['DTE'] = EOD_CSV['DTE'].fillna(0).astype('float64')\n",
    "                EOD_CSV['C_LAST'] = EOD_CSV['C_LAST'].fillna(0).astype('float64')\n",
    "    \n",
    "                # Convert the DataFrame to CSV format (without the index)\n",
    "                #csv_data = EOD_CSV.to_csv(index=False)\n",
    "                # Insert data into ClickHouse using the CSV format\n",
    "                clickhouse_client.insert('stagging.options_trading', EOD_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c9253b-a715-46dc-8bcb-bab787e2a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## DATAWAREHOUSE #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "52841679-36b7-4563-8cf7-3445a0a42328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Set] : [UNDERLYING_LAST] - StandardScaler() \n",
      "[Set] : [STRIKE] - StandardScaler() \n",
      "[Set] : [STRIKE_DISTANCE] - StandardScaler() \n",
      "[Set] : [INTRINSIC_VALUE] - StandardScaler() \n",
      "[Set] : [DTE] - StandardScaler() \n",
      "[Set] : [QUOTE] - StandardScaler() \n",
      "[Set] : [VEGA] - StandardScaler() \n",
      "[Set] : [VOLUME] - StandardScaler() \n"
     ]
    }
   ],
   "source": [
    "SCALER = Scaler(\n",
    "    [v for v in SCALER_COL if v not in QUOTE_COL+VEGA_COL+VOLUME_COL ] + ['QUOTE','VEGA','VOLUME']\n",
    "    , create=True\n",
    "    , path=\"./data/scaler/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9afbcad5-8ef0-4a65-8913-1b67de4ce1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query ClickHouse and load data into a Pandas DataFrame\n",
    "query = \"\"\"SELECT QUOTE_DATE,SYMBOL,EXPIRE_DATE FROM stagging.options_trading\n",
    "GROUP BY QUOTE_DATE,SYMBOL,EXPIRE_DATE\n",
    "ORDER BY SYMBOL,EXPIRE_DATE,QUOTE_DATE\"\"\"  # Replace 'your_table' with your table name\n",
    "data = clickhouse_client.query(query).result_rows  # Fetch data as a list of tuples\n",
    "column_names = clickhouse_client.query(query).column_names  # Fetch column names\n",
    "\n",
    "# Create a DataFrame from the result\n",
    "DF_UNIQUE_KEYS = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "MAX_OPTION_LEN = 16\n",
    "STRIKE_SELECT = []\n",
    "ID = None\n",
    "ref_key_symbol = None\n",
    "ref_key_expire = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4726fe28-0792-4081-bf6a-dfc2cfbc8b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,keys in tqdm( DF_UNIQUE_KEYS[:].iterrows() ,total = len(DF_UNIQUE_KEYS) ):\n",
    "    key_quote = keys['QUOTE_DATE'].strftime('%Y-%m-%d')\n",
    "    key_symbol = keys['SYMBOL']\n",
    "    key_expire = keys['EXPIRE_DATE'].strftime('%Y-%m-%d')\n",
    "    \n",
    "    query = f\"\"\"SELECT * FROM stagging.options_trading\n",
    "            WHERE\n",
    "            QUOTE_DATE = '{key_quote}'\n",
    "            AND SYMBOL = '{key_symbol}'\n",
    "            AND EXPIRE_DATE = '{key_expire}'\n",
    "            ORDER BY STRIKE  desc \"\"\" \n",
    "    data = clickhouse_client.query(query).result_rows  # Fetch data as a list of tuples\n",
    "    column_names = clickhouse_client.query(query).column_names  # Fetch column names\n",
    "    DF = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "    #select strike\n",
    "    if not len(STRIKE_SELECT)  or (key_symbol,key_expire) == (ref_key_symbol,ref_key_expire):\n",
    "        STRIKE_SELECT = DF[ DF['INTRINSIC_VALUE'].abs().isin(DF['INTRINSIC_VALUE'].abs().sort_values()[:MAX_OPTION_LEN]) ]['STRIKE'].values\n",
    "        ID = uuid.uuid4()\n",
    "        SCALER.save()\n",
    "        clear_output(wait = False)\n",
    "\n",
    "    rm_strike_index = DF[ ~DF['STRIKE'].isin(STRIKE_SELECT) ].index\n",
    "    DF = DF.drop(rm_strike_index)\n",
    "\n",
    "    #break loops when df < X\n",
    "    if len(DF) < 5 : \n",
    "        print(f\"\"\"{key_symbol}|{key_expire}|{key_quote} -> break (less rows)\"\"\")\n",
    "        ref_key_symbol = key_symbol\n",
    "        ref_key_expire = key_expire\n",
    "        continue \n",
    "        \n",
    "    if len(DF) == MAX_OPTION_LEN and not np.isnan( np.sum( DF[SCALER_COL].values ) ) :\n",
    "        DATA  = np.empty((0,) + ( MAX_OPTION_LEN,len(SCALER_COL) ) )\n",
    "        DATA = np.vstack((DATA ,[DF[SCALER_COL]]))\n",
    "        \n",
    "        if len(DATA) :\n",
    "            #========================== partial_fit ================================#\n",
    "            for col_i in SCALER_OTHER_COL_INDEX:\n",
    "                SCALER()[SCALER_COL[col_i]].partial_fit( DATA.reshape(-1, len(SCALER_COL))[:,[col_i]] )\n",
    "            SCALER()['QUOTE'].partial_fit( DATA.reshape(-1, len(SCALER_COL))[:,SCALER_QUOTE_COL].reshape(-1, 1) )\n",
    "            SCALER()['VEGA'].partial_fit( DATA.reshape(-1, len(SCALER_COL))[:,SCALER_VEGA_COL].reshape(-1, 1) )\n",
    "            SCALER()['VOLUME'].partial_fit( DATA.reshape(-1, len(SCALER_COL))[:,SCALER_VOLUME_COL].reshape(-1, 1) )\n",
    "            \n",
    "    DF['ID'] = ID\n",
    "    clickhouse_client.insert('warehouse.options_trading', DF)\n",
    "    ref_key_symbol = key_symbol\n",
    "    ref_key_expire = key_expire\n",
    "    \n",
    "SCALER.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fba95972-4fc1-4aa1-8761-cc08a3e545f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"SELECT * FROM stagging.options_trading\n",
    "        WHERE\n",
    "        QUOTE_DATE = '{key_quote}'\n",
    "        AND SYMBOL = '{key_symbol}'\n",
    "        AND EXPIRE_DATE = '{key_expire}'\n",
    "        ORDER BY STRIKE  desc \"\"\" \n",
    "data = clickhouse_client.query(query).result_rows  # Fetch data as a list of tuples\n",
    "column_names = clickhouse_client.query(query).column_names  # Fetch column names\n",
    "DF = pd.DataFrame(data, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f153661-3fb2-419c-8df5-c085634b0974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
